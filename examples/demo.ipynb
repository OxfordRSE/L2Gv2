{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from l2gv2.align.utils import to_device\n",
    "from scipy.stats import special_ortho_group\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from umap import UMAP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"grey\"> Graph Representation Learning at Scale</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"grey\">  Table of Contents</font>\n",
    "\n",
    "1. <a href='#chapter1'>Structure</a>\n",
    "2. <a href='#chapter2'>Datasets</a>\n",
    "3. <a href='#chapter3'>Graphs</a>\n",
    "4. <a href='#chapter4'>Patches</a>\n",
    "5. <a href='#chapter5'>Embedding</a>\n",
    "6. <a href='#chapter6'>Alignment</a>\n",
    "7. <a href='#chapter7'>Hierarchical alignment</a>\n",
    "8. <a href='#chapter8'>Visualisation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter1'> <font color=\"grey\">1. Structure </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five main parts to the package, organised as follows.\n",
    "\n",
    "```\n",
    "l2gv2/\n",
    "├── datasets/\n",
    "├── graphs/\n",
    "├── patch/\n",
    "├── embedding/\n",
    "└── align/\n",
    "    ├── l2g/\n",
    "    └── geo/\n",
    "```\n",
    "\n",
    "A brief overview of the contents:\n",
    "\n",
    "* ```datasets``` contains interfaces are provided for various common benchmark datasets. \n",
    "* ```graphs``` contains wrappers for graphs represented as lists of edges in pytorch-geometric ```data.edge_index``` format. These implemented features such as fast adjacency look-up and a variety of algorithms on graphs. Eventually, one could replace these with more sophisticad formats such as [Raphtory](https://www.raphtory.com/) graphs. \n",
    "* ```patch``` directory contains datastructures to represent patches and patch graphs, as well as methods to subdivide a graph into patches. \n",
    "* ```embedding``` contains various graph embedding methods, including Graph Autoencoders (GAE) and [Variational Graph Autoencoders (VGAE)](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.VGAE.html).\n",
    "* ```align``` contains two methods to compute the alignment of patches into a single graph embedding: eigenvalue synchronisation based on the [Local2Global](https://link.springer.com/article/10.1007/s10994-022-06285-7) algorithm, and the new method based on learning the alignment using a one-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter2'> <font color=\"grey\">2. Datasets </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are currently three datasets available:\n",
    "\n",
    "* ```Cora```: The [Cora dataset](https://graphsandnetworks.com/the-cora-dataset/) is a dataset of 2708 scientific publications divided into 7 classes. The citation network consists of 5429 directed edges. To each publication / node there is an associated 1433-dimensional feature vector indicating the presence or absence of certain words. This dataset is accessed through the pytorch-geometric [Planetoid](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html) dataset.\n",
    "* ```as-733```: The [SNAP autonomous systems AS-733](https://snap.stanford.edu/data/as-733.html) dataset contains 733 daily snapshots that span an interval of 785 days from November 8 1997 to January 2 2000. In each of these datasets, nodes represent autonomous systems and edges indicate whether communication has taken place. The resulting graph is undirected.\n",
    "* ```mag240```: The [MAT240M](https://ogb.stanford.edu/docs/lsc/mag240m/) dataset is a large heterogeneous academic citation graph.\n",
    "* ```elliptic```: The [Elliptic dataset](https://www.kaggle.com/datasets/ellipticco/elliptic-data-set) maps Bitcoin transactions to real entities belonging to licit categories (exchanges, wallet providers, miners, licit services, etc.) versus illicit ones (scams, malware, terrorist organizations, ransomware, Ponzi schemes, etc.). The task on the dataset is to classify the illicit and licit nodes in the graph. The graph consists of 203,769 nodes representing transactions and 234,355 directed edges representing payments flows.\n",
    "A case study is the paper [Anti-Money Laundering in Bitcoin: Experimenting with Graph\n",
    "Convolutional Networks for Financial Forensics](https://arxiv.org/pdf/1908.02591) by Weber et.al.\n",
    "* ```Dgraph```: [DGraph](https://dgraph.xinye.com/dataset) is a real world financial graph assembled for anomaly detection. This graph is described in the paper [DGraph: A Large-Scale Financial DAtaset for Graph Anomaly Detection](https://arxiv.org/abs/2207.03579).\n",
    "\n",
    "These are accessed using the ```get_dataset``` function. The format of the datasets follows closely the [pytorch-geometric convention](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Dataset.html). If the data is not locally available, it is retreived and preprocessed into a specified directory. In the case of temporal (dynamic) graphs, the graphs are returned as an iterable over time slices. Ultimately, the graphs can be exported into formats such as raphtory or used to initialize ```TGraph``` instances (see <a href='#chapter3'>Chapter 3: Graphs</a>). Currently, only ```Cora```, ```as-733``` (temporal) and ```DGraph``` are available. The ```DGraph``` dataset needs to be downloaded manually and a path to the zip file provided as argument when first initialising it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gv2.datasets import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading edge and node data from memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1cb5faa3294a03be5f54a0471f4971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), IntProgress(value=0, max=10556), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a41e7f38304b4083a6d1b754fa6986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), IntProgress(value=0, max=2708), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "cora = get_dataset(\"Cora\")\n",
    "print(cora[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First transform data into raphtory format, then networkx for plotting.\n",
    "G = cora.to(\"raphtory\").to_networkx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 10:48:52,200 - INFO - Downloading dataset tarball...\n",
      "2025-04-10 10:49:06,598 - INFO - Download complete.\n",
      "2025-04-10 10:49:06,599 - INFO - Extracting dataset tarball...\n",
      "2025-04-10 10:49:07,037 - INFO - Extraction complete.\n",
      "Processing...\n",
      "2025-04-10 10:49:07,038 - INFO - Processing raw text files into Polars DataFrames...\n",
      "2025-04-10 10:49:08,024 - INFO - Processing edges complete. Parquet files saved to /Users/u1774790/Projects/G2007/code/L2Gv2/data/as733/processed.\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading edge and node data from memory\n",
      "Loading edge and node data from memory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4febc2abc4b847e6aea2c5d30432c0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), IntProgress(value=0, max=11965533), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16fadfd791b4a7bab4a8ab6c487626d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), IntProgress(value=0, max=3066397), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "as733 = get_dataset(\"as-733\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats on the graph structure:\n",
      "Number of nodes (AS nodes): 7716\n",
      "Number of unique edges (src,dst): 45645\n",
      "Total interactions (edge updates): 11965533\n",
      "Stats on the graphs time range:\n",
      "Earliest datetime: 1997-11-08 00:00:00+00:00\n",
      "Latest datetime: 2000-01-02 00:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "g = as733.to(\"raphtory\")\n",
    "\n",
    "print(\"Stats on the graph structure:\")\n",
    "\n",
    "number_of_nodes = g.count_nodes()\n",
    "number_of_edges = g.count_edges()\n",
    "total_interactions = g.count_temporal_edges()\n",
    "\n",
    "print(\"Number of nodes (AS nodes):\", number_of_nodes)\n",
    "print(\"Number of unique edges (src,dst):\", number_of_edges)\n",
    "print(\"Total interactions (edge updates):\", total_interactions)\n",
    "\n",
    "print(\"Stats on the graphs time range:\")\n",
    "\n",
    "earliest_datetime = g.earliest_date_time\n",
    "latest_datetime = g.latest_date_time\n",
    "\n",
    "print(\"Earliest datetime:\", earliest_datetime)\n",
    "print(\"Latest datetime:\", latest_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter3'> <font color=\"grey\">3. Graphs </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three wrappers for graphs that were taken over from the local2global package: ```TGraph```, ```NPGraph``` and ```JitGraph```. These include, among other things, methods for fast adjacency look-up and various optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gv2.graphs import TGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,     3,     6,  ..., 10548, 10552, 10556])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "tg = TGraph(cora[0].edge_index, edge_attr=cora[0].edge_attr, x=cora[0].x)\n",
    "print(tg.adj_index)\n",
    "print(tg.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a future iteration one can think about consolidating this part by having graphs represented in some existing graph package like Raphtory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter4'> <font color=\"grey\">4. Patches </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A patch can equivalently refer to a subgraph or to an embedding of this subgraph. As a set of points, a patch is represented using the ```Patch``` class. A ```Patch``` object has the properties ```nodes```, ```index``` and ```coordinates```. ```nodes``` is simply a list of the nodes from the original graph that are present in the patch. ```index``` is a dict that maps each node to an index into ```coordinates```, which is just a list of coordinates. For example, if a graph embedding consists of four nodes in two dimensions as follows, and a patch is represented by the solid circles, then the corresponding object would have the following properties:\n",
    "\n",
    "![Patch](./images/square_patch.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gv2.patch.patches import Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Patch([0,2,3], np.array([[0., 0.], [1., 0.], [1., 1.]]))\n",
    "print(p.coordinates)\n",
    "print(p.nodes)\n",
    "print(p.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gv2.example import generate_patches, random_transform_patches, plot_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = generate_patches(n_points = 200, n_clusters=10)\n",
    "n_patches = len(patches)\n",
    "print(f\"First 5 coordinates: {patches[0].coordinates[:5]}\")\n",
    "print(f\"First 5 nodes: {patches[0].nodes[:5]}\")\n",
    "print(f\"Indices: {patches[0].index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crucial feature of patches is that different patches may reference the same node in an underlying graph, but with different coordiantes. A typical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply random shifts and rotations to the patches\n",
    "transformed_patches = random_transform_patches(patches)\n",
    "plot_patches(patches, transformed_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The patches from the nodes of a **patch graph**, where two nodes are connected by an edge if the patches contain overlapping nodes. The alignment tasks consists of making the correponding coordinates overlap as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter5'> <font color=\"grey\">5. Alignment </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a way of representing patches and a way of embedding them. The next step is to compute the alignment based on the patch graph. There are two methods for this, we focus on the 'new' one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # For reproducibility\n",
    "points = np.random.rand(100, 2)\n",
    "patches = [Patch([i for i in range(60)], points[:60]),\n",
    "           Patch([i for i in range(40,100)], points[-60:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(patches[0].coordinates[:, 0], patches[0].coordinates[:, 1], c='blue', alpha=0.7, label='Patch 1')\n",
    "plt.scatter(patches[1].coordinates[:, 0], patches[1].coordinates[:, 1], c='red', alpha=0.7, label='Patch 2')\n",
    "plt.legend()\n",
    "overlap_indices = list(set(patches[0].nodes.tolist()).intersection(set(patches[1].nodes.tolist())))\n",
    "print(overlap_indices)\n",
    "if overlap_indices:\n",
    "    overlap_coords_patch0 = patches[0].get_coordinates(overlap_indices)\n",
    "    overlap_coords_patch1 = patches[1].get_coordinates(overlap_indices)\n",
    "    plt.scatter(overlap_coords_patch0[:, 0], overlap_coords_patch0[:, 1], c='green', s=100, alpha=0.7, label='Overlap')\n",
    "plt.scatter(overlap_coords_patch0[:, 0], overlap_coords_patch0[:, 1], c='green', s=100, alpha=0.7, label='Overlap')  # Increased size from 100 to 200\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation = special_ortho_group.rvs(2)  # 2x2 random orthogonal matrix\n",
    "scale = np.random.uniform(0.5, 2.0)\n",
    "translation = np.random.uniform(-5, 5, size=2)\n",
    "set2_transformed = scale * (patches[1].coordinates @ rotation.T) + translation\n",
    "transformed_patches = [patches[0], Patch(patches[1].nodes, set2_transformed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(transformed_patches[0].coordinates[:, 0], transformed_patches[0].coordinates[:, 1], c='blue', alpha=0.7, label='Patch 1')\n",
    "plt.scatter(transformed_patches[1].coordinates[:, 0], transformed_patches[1].coordinates[:, 1], c='red', alpha=0.7, label='Patch 2')\n",
    "plt.legend()\n",
    "overlap_indices = list(set(transformed_patches[0].nodes.tolist()).intersection(set(transformed_patches[1].nodes.tolist())))\n",
    "print(overlap_indices)\n",
    "if overlap_indices:\n",
    "    overlap_coords_patch0 = transformed_patches[0].get_coordinates(overlap_indices)\n",
    "    overlap_coords_patch1 = transformed_patches[1].get_coordinates(overlap_indices)\n",
    "    plt.scatter(overlap_coords_patch0[:, 0], overlap_coords_patch0[:, 1], c='green', s=100, alpha=0.7, label='Overlap')\n",
    "    plt.scatter(overlap_coords_patch1[:, 0], overlap_coords_patch1[:, 1], c='green', s=100, alpha=0.7, label='Overlap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the patch graph, we first determine the intersections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersections(patches, min_overlap=0):\n",
    "    \"\"\"Calculate the intersection of nodes between patches.\"\"\"\n",
    "    intersections = {}\n",
    "    embeddings = {}\n",
    "    for i, _ in enumerate(patches):\n",
    "        for j in range(i + 1, len(patches)):\n",
    "            intersections[(i, j)] = list(\n",
    "                set(patches[i].nodes.tolist()).intersection(\n",
    "                    set(patches[j].nodes.tolist())\n",
    "                )\n",
    "            )\n",
    "            if len(intersections[(i, j)]) >= min_overlap:\n",
    "                embeddings[(i, j)] = [\n",
    "                    torch.tensor(\n",
    "                        patches[i].get_coordinates(list(intersections[(i, j)]))\n",
    "                    ),\n",
    "                    torch.tensor(\n",
    "                        patches[j].get_coordinates(list(intersections[(i, j)]))\n",
    "                    ),\n",
    "                ]\n",
    "    # embeddings = list(itertools.chain.from_iterable(embeddings))\n",
    "    return intersections, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections, embeddings = get_intersections(transformed_patches, min_overlap=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only one edge connecting the two patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model for aligning patch embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, n_patches, device):\n",
    "        \"\"\"\n",
    "        Initialize the model\n",
    "        Args:\n",
    "            dim: int\n",
    "            n_patches: int\n",
    "            device: str\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        linear_layers = [nn.Linear(dim, dim, bias=True).to(device) for _ in range(n_patches)]\n",
    "        # Fix the first transformation to be the identity\n",
    "        fixed_layer_index = 0\n",
    "        linear_layers[fixed_layer_index].weight.data.copy_(torch.eye(dim))\n",
    "        linear_layers[fixed_layer_index].bias.data.zero_()\n",
    "        linear_layers[fixed_layer_index].weight.requires_grad = False\n",
    "        linear_layers[fixed_layer_index].bias.requires_grad = False\n",
    "\n",
    "        self.transformation = nn.ParameterList(linear_layers)\n",
    "\n",
    "    def forward(self, patch_intersection):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \"\"\"\n",
    "        outputs = {}\n",
    "        for (i, j), (X, Y) in patch_intersection.items():\n",
    "            Xt = self.transformation[i](X)\n",
    "            Yt = self.transformation[j](Y)\n",
    "            outputs[(i, j)] = (Xt, Yt)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchgraph_mse_loss(transformed_emb):\n",
    "    total_loss = 0.0\n",
    "    for (_, _), (transformed_X, transformed_Y) in transformed_emb.items():\n",
    "        pair_loss = F.mse_loss(transformed_X, transformed_Y, reduction=\"sum\")\n",
    "        total_loss += pair_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "device = \"cpu\"\n",
    "model = AffineModel(dim, n_patches, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-1)\n",
    "loss_hist = []\n",
    "patch_emb = to_device(embeddings, \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    transformed_patch_emb = model(patch_emb)\n",
    "    loss = patchgraph_mse_loss(transformed_patch_emb)\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    loss_hist.append(loss.item())\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoverd_patches_coordinates = [\n",
    "    transformed_patches[i].coordinates @ model.transformation[i].weight.data.detach().numpy().T\n",
    "    + model.transformation[i].bias.data.detach().numpy()\n",
    "    for i in range(2)\n",
    "]\n",
    "recoverd_patches = [Patch(transformed_patches[i].nodes, recoverd_patches_coordinates[i]) for i in range(2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(recoverd_patches[0].coordinates[:, 0], recoverd_patches[0].coordinates[:, 1], c='blue', alpha=0.7, label='Patch 1')\n",
    "plt.scatter(recoverd_patches[1].coordinates[:, 0], recoverd_patches[1].coordinates[:, 1], c='red', alpha=0.7, label='Patch 2')\n",
    "plt.legend()\n",
    "overlap_indices = list(set(recoverd_patches[0].nodes.tolist()).intersection(set(recoverd_patches[1].nodes.tolist())))\n",
    "print(overlap_indices)\n",
    "if overlap_indices:\n",
    "    overlap_coords_patch0 = recoverd_patches[0].get_coordinates(overlap_indices)\n",
    "    overlap_coords_patch1 = recoverd_patches[1].get_coordinates(overlap_indices)\n",
    "    plt.scatter(overlap_coords_patch0[:, 0], overlap_coords_patch0[:, 1], c='green', s=100, alpha=0.7, label='Overlap')\n",
    "    plt.scatter(overlap_coords_patch1[:, 0], overlap_coords_patch1[:, 1], c='green', s=100, alpha=0.7, label='Overlap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gv2.align import get_aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = generate_patches(n_points = 200, n_clusters=10)\n",
    "n_patches = len(patches)\n",
    "transformed_patches = random_transform_patches(patches)\n",
    "plot_patches(patches, transformed_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_aligner = get_aligner(\n",
    "    \"geo\", \n",
    "    patches=transformed_patches,\n",
    "    num_epochs=500,\n",
    "    learning_rate=0.1,\n",
    "    model_type=\"affine\")\n",
    "embedding = geo_aligner.get_aligned_embedding()\n",
    "plot_patches(geo_aligner.patches, patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(geo_aligner.loss_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter6'> <font color=\"grey\">6. Embedding </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coordinates of a patch come from an embedding of a graph into Euclidean space. \n",
    "For the embedding, we use the architecture of a Variational Graph Autoencoder. Given a graph $G=(V,E)$ with $|V|=n$ nodes and node features ${x}_i\\in \\mathbb{R}^d$, $i\\in [n]$, denote by ${X}=[{x}_1,\\dots,{x}_n]^T\\in \\mathbb{R}^{n\\times d}$ the features matrix and by $A=(a_{ij})\\in \\{0,1\\}^{n\\times n}$ the adjacency matrix of the graph. The **encoder** produces latent representations ${z}_i\\in \\mathbb{R}^k$ for $i\\in [n]$, which are sampled from the inference model\n",
    "\\begin{equation*}\n",
    "  q({z}_i \\ | \\ {X},{A}) = \\mathcal{N}({z}_i \\ | \\ {\\mu}_i,\\mathrm{diag}({\\sigma}_i)).\n",
    "\\end{equation*}\n",
    "The means $\\mu_i$ and variances $\\mathrm{diag}({\\sigma}_i)$ are parametrized using an encoder network, for example, a graph convolutional neural network (GCN). Denoting by ${Z}=[{z}_1,\\dots,{z}_n]^T$ the matrix of latent represenations and by ${\\mu}$ and ${\\sigma}$ the matrices representing the means and variances, we have\n",
    "\\begin{equation*}\n",
    "  {\\mu} = \\mathrm{GCN}_{\\mu}({X},{A}), \\quad \\quad \\log {\\sigma} = \\mathrm{GCN}_{\\sigma}({X},{A}).\n",
    "\\end{equation*}\n",
    "The **generative model** is a distribution on the adjacency matrix,\n",
    "\\begin{equation*}\n",
    "  p({A}\\ | \\ {Z}) = \\prod_{i,j} p(a_{ij} \\ | \\ {z}_i,{z}_j).\n",
    "\\end{equation*}\n",
    "It is convenient to use\n",
    "\\begin{equation*}\n",
    "  p(a_{ij}=1 \\ | \\ {z}_i,{z}_j) = \\sigma({z}_i^T{z}_j),\n",
    "\\end{equation*}\n",
    "where $\\sigma$ is the logistic sigmoid. In order to train the model, we optimize the evidence lower bound\n",
    "\\begin{equation*}\n",
    "  \\mathcal{L} = \\mathbb{E}_{q({Z}\\ | \\ {X},{A})}[\\log p({A}\\ | \\ {Z})]-\\mathrm{D}_{\\mathrm{KL}}(q({Z}\\ | \\ {X},{A}) \\ \\| \\ p({Z})).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gv2.embedding.gae import VGAE\n",
    "from l2gv2.embedding.train import train\n",
    "from l2gv2.embedding.gae.utils.loss import VGAE_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGAE(dim=64, hidden_dim=128, num_features=tg.x.shape[1]).to(device)\n",
    "model = train(cora[0], model, loss_fun=VGAE_loss, num_epochs=200, verbose=False, lr=0.001)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    coordinates = model.encode(cora[0]).to(\"cpu\").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = UMAP(n_components=2, random_state=42)\n",
    "umap_data = reducer.fit_transform(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(umap_data[:, 0], umap_data[:, 1], c=cora[0].y, cmap='viridis', alpha=0.5)\n",
    "plt.title('UMAP Visualization Cora data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter7'> <font color=\"grey\">7. Hierarchical alignment </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter8'> <font color=\"grey\">8. Visualisation </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the visualisation, it is convenient to use external packages such as Heimdall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
