{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import coalesce\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_dir = \"./data\"\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l2g related imports\n",
    "from l2gv2.patch.clustering import louvain_clustering\n",
    "from l2gv2.graphs.tgraph import TGraph\n",
    "from l2gv2.graphs.npgraph import NPGraph\n",
    "from l2gv2.patch.patches import create_patch_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"grey\"> Local2Global 2.0</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter1'> <font color=\"grey\">1. The local2global approach </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial step in creating the patch graph consists of clustering the graph. The clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cora dataset is a well-known dataset in the field of graph research. This consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gv2.datasets import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "planetoid_cora() missing 1 required positional argument: 'root'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m get_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCora\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/Graphs/code/L2Gv2/l2gv2/datasets/registry.py:40\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m(name, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     available \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(DATASET_REGISTRY\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not registered. Available datasets: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DATASET_REGISTRY[name](\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: planetoid_cora() missing 1 required positional argument: 'root'"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(\"Cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = to_networkx(data, to_undirected=True)\n",
    "degrees = [val for (node, val) in G.degree()]\n",
    "display(pd.DataFrame(pd.Series(degrees).describe()).transpose().round(2))\n",
    "print(len(degrees))\n",
    "print(sum(degrees))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(degrees, bins=50)\n",
    "plt.xlabel(\"node degree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = to_networkx(data, to_undirected=True)\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "cent = nx.degree_centrality(G)\n",
    "node_size = list(map(lambda x: x * 500, cent.values()))\n",
    "cent_array = np.array(list(cent.values()))\n",
    "threshold = sorted(cent_array, reverse=True)[10]\n",
    "print(\"threshold\", threshold)\n",
    "cent_bin = np.where(cent_array >= threshold, 1, 0.1)\n",
    "plt.figure(figsize=(12, 12))\n",
    "nodes = nx.draw_networkx_nodes(G, pos, node_size=node_size,\n",
    "                               cmap=plt.cm.plasma,\n",
    "                               node_color=cent_bin,\n",
    "                               nodelist=list(cent.keys()),\n",
    "                               alpha=cent_bin)\n",
    "edges = nx.draw_networkx_edges(G, pos, width=0.25, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    0: \"Theory\",\n",
    "    1: \"Reinforcement_Learning\",\n",
    "    2: \"Genetic_Algorithms\",\n",
    "    3: \"Neural_Networks\",\n",
    "    4: \"Probabilistic_Methods\",\n",
    "    5: \"Case_Based\",\n",
    "    6: \"Rule_Learning\"}\n",
    "data.y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspath = Path(\"../data/snap-as/as_edges.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(aspath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters: \n",
    "# 10 patches\n",
    "# Average degree k=4\n",
    "# Overlap between 256 and 1024\n",
    "# Embedding dimension up to 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_cluster_and_embed(graph: nx.Graph, m: int, k: int) -> List[Tuple[nx.Graph, List[float]]]:\n",
    "    if graph.number_of_nodes() <= m:\n",
    "        return [(graph, embed(graph))]\n",
    "    \n",
    "    clusters = cluster(graph, k)\n",
    "    results = []\n",
    "    \n",
    "    for subgraph in clusters:\n",
    "        results.extend(hierarchical_cluster_and_embed(subgraph, m, k))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = data.edge_index.numpy()\n",
    "print(edge_index.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
